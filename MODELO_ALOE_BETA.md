# Modelo Aloe-Beta-v3 (Ollama)

## Resumen

**Escriba M√©dico** utiliza **Aloe-Beta-8B** como su generador de notas SOAP. Este modelo es:

- **Desarrollado por:** Barcelona Supercomputing Center (BSC-CNS)
- **Base:** Llama 3.1 Fine-tuned en espa√±ol m√©dico
- **Cuantizaci√≥n:** Q4_K_M (GGUF)
- **Ejecutable:** Localmente v√≠a Ollama (sin depender de APIs externas)

## Instalaci√≥n y Configuraci√≥n

### 1. Descargar e Instalar Ollama

Si a√∫n no tienes Ollama, desc√°rgalo desde: https://ollama.com/download

Una vez instalado, ejecuta el servicio:

```powershell
ollama serve
```

### 2. Hacer Pull del Modelo Aloe-Beta-8B

En otra terminal, ejecuta:

```powershell
ollama pull hf.co/mradermacher/Llama3.1-Aloe-Beta-8B-GGUF:Q4_K_M
```

Esto descargar√° autom√°ticamente el modelo (aprox. 5-7 GB).

### 2b. (IMPORTANTE) Crear Modelo Personalizado con Prompt M√©dico

Este proyecto incluye un `Modelfile` con el prompt especializado en medicina. Para cargarlo:

```powershell
ollama create escriba-aloe-v3 -f Modelfile
```

Este comando crea un modelo llamado `escriba-aloe-v3` que:
- ‚úÖ Usa Aloe-Beta-8B como base
- ‚úÖ Integra el prompt m√©dico especializado (REGLAS DE ORO para SOAP)
- ‚úÖ Configura par√°metros √≥ptimos (temperatura=0.05 para precisi√≥n)

**Nota:** La app usar√° autom√°ticamente este modelo si lo creas. Si no lo creas, utilizar√° el modelo base sin el prompt personalizado.

## Verificar el Modelo

Desde la app:

1. Abre Streamlit: `streamlit run app.py`
2. Expande la barra lateral: **üß† Modelo local (Ollama)**
3. Si ves "‚úÖ Conectado a Ollama", el servicio est√° activo
4. Selecciona el modelo en el dropdown (por defecto: `escriba-aloe-v3` o `hf.co/mradermacher/Llama3.1-Aloe-Beta-8B-GGUF:Q4_K_M`)

## Requisitos de Sistema

- **RAM:** 16 GB (m√≠nimo), 32+ recomendado
- **GPU:** NVIDIA con CUDA (opcional pero recomendado para velocidad)
- **Disco:** 10-15 GB libres
- **Ancho de banda:** Para la descarga inicial

## Caracter√≠sticas M√©dicas

El modelo **escriba-aloe-v3** (creado con nuestro Modelfile) est√° entrenado/configurado para:

- ‚úÖ Generar notas SOAP estructuradas en espa√±ol
- ‚úÖ Aplicar **REGLAS DE ORO** para m√°xima precisi√≥n cl√≠nica
- ‚úÖ Preservar terminolog√≠a m√©dica exacta (tecnicismos)
- ‚úÖ Capturar n√∫meros precisos (TA, FC, dosis, tiempos)
- ‚úÖ Entender contexto rural y geri√°trico
- ‚úÖ Sugerir diagn√≥sticos candidatos con nivel de probabilidad
- ‚úÖ Mantener baja temperatura (0.05) para determinismo

**Sistema Prompt:** Incluye 6 secciones + Reglas de Oro para evitar alucinaciones.

## Notas T√©cnicas

- **Cuantizaci√≥n Q4_K_M:** Reduce tama√±o sin perder precisi√≥n significativa
- **Latencia:** ~5-10 seg. por informe en GPU NVIDIA, ~30-60 seg. en CPU
- **Acceso:** El modelo NO se puede acceder desde internet; corre 100% localmente

## Troubleshooting

- **Error: "ollama command not found"**
  - Verifica que Ollama est√© en PATH o usa la ruta completa: `C:\Program Files\Ollama\ollama.exe serve`

- **Error: "Model not found"**
  - Ejecuta: `ollama pull hf.co/mradermacher/Llama3.1-Aloe-Beta-8B-GGUF:Q4_K_M`

- **Respuesta vac√≠a o lenta**
  - Verifica disponibilidad de RAM y GPU
  - Comprueba que el puerto 11434 (Ollama) no est√© bloqueado

## Referencias

- Aloe-Beta: https://huggingface.co/BSC-LT/Aloe-Llama-3.1-8B
- Ollama: https://ollama.com
- GGUF Format: https://github.com/ggerganov/ggml
